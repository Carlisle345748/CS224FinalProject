# -*- coding: utf-8 -*-
"""ABS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yQk2WYb86B_A8RYTgeIcgsn-JQvqnTkM
"""

import random

import pandas as pd
import torch
from datasets import load_from_disk
from gensim.summarization.bm25 import BM25
from torch.nn.functional import normalize
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("Luyu/co-condenser-marco")
model = AutoModel.from_pretrained('Luyu/co-condenser-marco')
similarity_score = {}

samples = load_from_disk("Dataset/sample_50000_raw")
data = [(x['query'], x['positives']) for x in samples]
passage = list(set([x['positives'] for x in samples]))
p2idx = {passage[i]: i for i in range(len(passage))}
bm25 = BM25([doc.split(" ") for doc in passage])


def get_similarity(
		query_text,
		passage_text,
		encode=False
):
	if encode:
		model.eval()
		sequences = [query_text, passage_text]
		model_inputs = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
		encoding = model(**model_inputs, return_dict=True).last_hidden_state[:, 0]
		e1 = torch.nn.functional.normalize(encoding[0], p=2.0, dim=0)
		e2 = torch.nn.functional.normalize(encoding[1], p=2.0, dim=0)
		similarity = e1 @ e2

	else:
		query_tokens = query_text.split()
		similarity = bm25.get_score(query_tokens, p2idx[passage_text])

	return similarity


def get_hardness(
		hardness,
		dataset,
		d_remove=None,
		d_add=None,
		encode=False
):
	temp = dataset.copy()

	if d_remove is None and d_add is None:
		for pair1 in temp:
			for pair2 in temp:
				if pair1 != pair2:
					if (pair1[0], pair2[1]) not in similarity_score:
						similarity_score[(pair1[0], pair2[1])] = get_similarity(pair1[0], pair2[1], encode)
					hardness = hardness + similarity_score[(pair1[0], pair2[1])]

	elif d_remove is not None and d_add is not None:
		for pair in dataset:
			if pair != d_remove:
				if (pair[0], d_remove[1]) not in similarity_score:
					similarity_score[(pair[0], d_remove[1])] = get_similarity(pair[0], d_remove[1], encode)
				if (d_remove[0], pair[1]) not in similarity_score:
					similarity_score[(d_remove[0], pair[1])] = get_similarity(d_remove[0], pair[1], encode)
				if (pair[0], d_add[1]) not in similarity_score:
					similarity_score[(pair[0], d_add[1])] = get_similarity(pair[0], d_add[1], encode)
				if (d_add[0], pair[1]) not in similarity_score:
					similarity_score[(d_add[0], pair[1])] = get_similarity(d_add[0], pair[1], encode)
				hardness = hardness - similarity_score[(pair[0], d_remove[1])] - similarity_score[
					(d_remove[0], pair[1])] + similarity_score[(pair[0], d_add[1])] + similarity_score[
					           (d_add[0], pair[1])]

	elif d_remove is not None and d_add is None:
		for pair in dataset:
			if pair != d_remove:
				if (pair[0], d_remove[1]) not in similarity_score:
					similarity_score[(pair[0], d_remove[1])] = get_similarity(pair[0], d_remove[1], encode)
				if (d_remove[0], pair[1]) not in similarity_score:
					similarity_score[(d_remove[0], pair[1])] = get_similarity(d_remove[0], pair[1], encode)
				hardness = hardness - similarity_score[(pair[0], d_remove[1])] - similarity_score[
					(d_remove[0], pair[1])]

	elif d_remove is None and d_add is not None:
		for pair in dataset:
			if (pair[0], d_add[1]) not in similarity_score:
				similarity_score[(pair[0], d_add[1])] = get_similarity(pair[0], d_add[1], encode)
			if (d_add[0], pair[1]) not in similarity_score:
				similarity_score[(d_add[0], pair[1])] = get_similarity(d_add[0], pair[1], encode)
			hardness = hardness + similarity_score[(pair[0], d_add[1])] + similarity_score[(d_add[0], pair[1])]

	return hardness


def get_remove(dataset, hardness):
	max_hardness = float("-inf")
	temp = dataset.copy()
	pair_remove = ()

	for pair in dataset:
		temp_hardness = get_hardness(hardness, temp, pair, None)
		if temp_hardness > max_hardness:
			max_hardness = temp_hardness
			pair_remove = pair

	return pair_remove


def get_add(current_dataset, pair_remove, remain_dataset, hardness):
	temp = current_dataset.copy()
	remain = remain_dataset.copy()
	remain = remain - set(temp)
	cur_hardness = get_hardness(hardness, temp, pair_remove, None)
	temp.remove(pair_remove)
	pair_add = ()
	max_hardness = float("-inf")

	for pair in remain:
		temp_hardness = get_hardness(cur_hardness, temp, None, pair)
		if temp_hardness > max_hardness:
			max_hardness = temp_hardness
			pair_add = pair

	return pair_add


def reformat(dataset):
	B = []

	for pair1 in dataset:
		query = pair1[0]
		current_row = []
		for pair2 in dataset:
			if pair2[0] != query:
				current_row.append(pair2[1])
		current_row = [pair1[1]] + current_row
		B.append([query, current_row])

	return B


def AdaptiveBatchSampling(dataset, batch_size):
	U = set(dataset)
	T = []

	while U:
		B = random.sample(U, batch_size)
		if len(U) > batch_size:
			hardness_B = get_hardness(0, B)
			while True:
				d_r = get_remove(B, hardness_B)
				d_a = get_add(B, d_r, U, hardness_B)
				hardness_B_tem = get_hardness(hardness_B, B, d_r, d_a)
				if hardness_B_tem > hardness_B:
					B.remove(d_r)
					B.append(d_a)
					hardness_B = hardness_B_tem
				else:
					break

		print(B)
		B = set(B)
		U = U - B
		T.extend(reformat(B))
	return T


if __name__ == '__main__':
	batches = AdaptiveBatchSampling(data, 8)
	df = pd.DataFrame.from_records(batches)
	df.columns = ['query', 'passage']
	df.to_json("sample_50000_abs.jsonl", orient="records", lines=True)
